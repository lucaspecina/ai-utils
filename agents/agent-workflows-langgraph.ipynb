{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflows and Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks: The Augmented LLM\n",
    "\n",
    "LLM have augmentations that support building workflows and agents. These include structured outputs and tool calling, as shown in this image from the Anthropic blog:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt chaining\n",
    "In prompt chaining, each LLM call processes the output of the previous one.\n",
    "\n",
    "As noted in the Anthropic blog:\n",
    "\n",
    "Prompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks (see \"gate” in the diagram below) on any intermediate steps to ensure that the process is still on track.\n",
    "\n",
    "When to use this workflow: This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "# _set_env(\"ANTHROPIC_API_KEY\")\n",
    "# _set_env(\"OLLAMA_API_KEY\")\n",
    "\n",
    "# llm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n",
    "# llm = ChatOllama(model=\"deepseek-r1:8b\")\n",
    "llm = ChatOllama(model=\"llama3.2:latest\") # porque tiene tool-calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 2, 'b': 3},\n",
       "  'id': 'dc04d9a9-96cb-4891-8eea-2067e49e606e',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Schema for structured output\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: str = Field(None, description=\"Query that is optimized web search.\")\n",
    "    justification: str = Field(\n",
    "        None, description=\"Why this query is relevant to the user's request.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "structured_llm = llm.with_structured_output(SearchQuery)\n",
    "\n",
    "# Invoke the augmented LLM\n",
    "output = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\n",
    "\n",
    "# Define a tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    return a * b\n",
    "\n",
    "# Augment the LLM with tools\n",
    "llm_with_tools = llm.bind_tools([multiply])\n",
    "\n",
    "# Invoke the LLM with input that triggers the tool call\n",
    "msg = llm_with_tools.invoke(\"What is 2 times 3?\")\n",
    "\n",
    "# Get the tool call\n",
    "msg.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generate_joke': 'Why did the cat join a band?\\n\\nBecause it wanted to be the purr-cussionist!'}\n",
      "\n",
      "\n",
      "{'improve_joke': 'Here\\'s an attempt to add some wordplay to make it even funnier:\\n\\nWhy did the cat join a band?\\n\\nBecause it wanted to be the purr-cussionist and really hammer out its own beat!\\n\\nThe added phrase \"hammer out its own beat\" incorporates the common idiomatic expression \"hammer out\" which means to create or produce something, while also referencing the physical action of a drummer hammering on drums. This adds a layer of wordplay to the joke, making it even more clever and amusing!'}\n",
      "\n",
      "\n",
      "{'polish_joke': \"Here's an attempt at adding a surprising twist to the joke:\\n\\nWhy did the cat join a band?\\n\\nBecause it wanted to be the purr-cussionist and really hammer out its own beat... but little did anyone know, it was actually a former intergalactic cat named Whiskers who had been sent back in time from a distant planet to study the art of rhythm. Its ancestors were legendary for their ability to create sonic blasts that could shatter asteroids, and now it wanted to use its newfound musical talents to blast off into a career as a rock star!\\n\\nThe added twist adds a sci-fi element to the joke, subverting the expectation that the cat is just joining a band for fun. Instead, it reveals a deeper purpose behind its actions, adding a layer of surprise and playfulness to the punchline!\"}\n",
      "\n",
      "\n",
      "{'parallel_workflow': \"Here's an attempt at adding a surprising twist to the joke:\\n\\nWhy did the cat join a band?\\n\\nBecause it wanted to be the purr-cussionist and really hammer out its own beat... but little did anyone know, it was actually a former intergalactic cat named Whiskers who had been sent back in time from a distant planet to study the art of rhythm. Its ancestors were legendary for their ability to create sonic blasts that could shatter asteroids, and now it wanted to use its newfound musical talents to blast off into a career as a rock star!\\n\\nThe added twist adds a sci-fi element to the joke, subverting the expectation that the cat is just joining a band for fun. Instead, it reveals a deeper purpose behind its actions, adding a layer of surprise and playfulness to the punchline!\"}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langgraph.func import entrypoint, task\n",
    "\n",
    "\n",
    "# Tasks\n",
    "@task\n",
    "def generate_joke(topic: str):\n",
    "    \"\"\"First LLM call to generate initial joke\"\"\"\n",
    "    msg = llm.invoke(f\"Write a short joke about {topic}\")\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "def check_punchline(joke: str):\n",
    "    \"\"\"Gate function to check if the joke has a punchline\"\"\"\n",
    "    # Simple check - does the joke contain \"?\" or \"!\"\n",
    "    if \"?\" in joke or \"!\" in joke:\n",
    "        return \"Fail\"\n",
    "\n",
    "    return \"Pass\"\n",
    "\n",
    "\n",
    "@task\n",
    "def improve_joke(joke: str):\n",
    "    \"\"\"Second LLM call to improve the joke\"\"\"\n",
    "    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {joke}\")\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "@task\n",
    "def polish_joke(joke: str):\n",
    "    \"\"\"Third LLM call for final polish\"\"\"\n",
    "    msg = llm.invoke(f\"Add a surprising twist to this joke: {joke}\")\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "@entrypoint()\n",
    "def parallel_workflow(topic: str):\n",
    "    original_joke = generate_joke(topic).result()\n",
    "    if check_punchline(original_joke) == \"Pass\":\n",
    "        return original_joke\n",
    "\n",
    "    improved_joke = improve_joke(original_joke).result()\n",
    "    return polish_joke(improved_joke).result()\n",
    "\n",
    "# Invoke\n",
    "for step in parallel_workflow.stream(\"cats\", stream_mode=\"updates\"):\n",
    "    print(step)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelization\n",
    "\n",
    "With parallelization, LLMs work simultaneously on a task:\n",
    "\n",
    "LLMs can sometimes work simultaneously on a task and have their outputs aggregated programmatically. This workflow, parallelization, manifests in two key variations: Sectioning: Breaking a task into independent subtasks run in parallel. Voting: Running the same task multiple times to get diverse outputs.\n",
    "\n",
    "When to use this workflow: Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations, LLMs generally perform better when each consideration is handled by a separate LLM call, allowing focused attention on each specific aspect.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'call_llm_1': 'Why did the cat join a band?\\n\\nBecause it wanted to be the purr-cussionist.'}\n",
      "\n",
      "\n",
      "{'call_llm_3': \"Whiskered shadows dance and play,\\nMoonlight whispers, night's sweet sway,\\nEyes like lanterns in the dark,\\nWatching worlds with secrets stark.\\n\\nSoft purrs rumble, velvet tone,\\nA soothing hum, a heart of stone,\\nTail twitches, ears perk high,\\nListening to the world go by.\\n\\nClaws unsheathed, a silent creep,\\nStalking prey in endless sleep,\\nThe huntress stirs, eyes aglow,\\nMidnight hunter, fierce and slow.\\n\\nTheir fur, a tapestry so fine,\\nPatterns woven like a shrine,\\n Colors shifting with each mood,\\nShades of gray, or eyes of gold.\\n\\nWith every step, a secret keeps,\\nA world of mystery, it seems to sleep,\\nBut when they curl up tight and still,\\nTheir hearts beat fast, their love unfulfilled.\\n\\nSo let us cherish these feline friends,\\nAnd honor the secrets that never end,\\nFor in their eyes, our souls are seen,\\nAnd in their purrs, a love is gleaned.\"}\n",
      "\n",
      "\n",
      "{'call_llm_2': 'In the quaint town of Whiskerville, there lived a community of felines who were known for their unique personalities and characteristics. Among them was a sleek black cat named Midnight, whose eyes shone like stars in the night sky.\\n\\nMidnight lived with her owner, an elderly woman named Mrs. Jenkins, in a cozy little house on the outskirts of town. Mrs. Jenkins adored Midnight and spoiled her rotten, feeding her treats and giving her endless belly rubs. In return, Midnight provided affection and companionship to Mrs. Jenkins, who had grown lonely after losing her husband.\\n\\nOne day, while exploring the attic of their home, Midnight stumbled upon an old trunk that had been hidden away for years. As she pawed at the lid, it creaked open, revealing a trove of forgotten treasures: glittering jewels, antique toys, and even an old catnip-filled toy mouse.\\n\\nAs Midnight began to investigate each item, she discovered that they were all connected to her own lineage. It turned out that Mrs. Jenkins\\' great-aunt had been a renowned cat breeder, and the trunk was filled with heirlooms from some of the most esteemed feline families in Whiskerville.\\n\\nMidnight\\'s curiosity was piqued, and she set out on a journey to explore the town and uncover more secrets about her ancestors. She began by visiting the local animal shelter, where she met a scrappy little calico named Ginger who claimed to have known Midnight\\'s great-great-grandmother.\\n\\nGinger told Midnight that their shared ancestor had been a fearless huntress, able to catch even the most elusive prey in the forest. Intrigued, Midnight continued her quest for knowledge, seeking out other felines with similar stories and experiences.\\n\\nHer travels took her through alleys, backyards, and hidden gardens, where she encountered an array of cats who shared tales of their own families\\' histories. There was Rufus, a majestic Maine Coon with piercing blue eyes; Luna, a gentle Persian with a silky coat and melodious voice; and even Jasper, a mischievous Siamese with a penchant for playing pranks on his owners.\\n\\nAs Midnight listened to each cat\\'s stories, she began to piece together a rich tapestry of Whiskerville\\'s feline heritage. She discovered that her ancestors had played a significant role in shaping the town\\'s history and culture, from their skillful hunting prowess to their expertise in weaving and craftsmanship.\\n\\nOver time, Midnight became known as the \"Whisker Historian,\" a title bestowed upon her by the cats of Whiskerville who revered her for her dedication to preserving their shared heritage. And with each new discovery, she grew more confident and curious, ready to uncover even more secrets about her own mysterious past.\\n\\nOne day, as Midnight curled up in Mrs. Jenkins\\' lap, she purred contentedly, knowing that she was part of a long line of remarkable cats who had shaped the town\\'s history and identity. And as she drifted off to sleep, her eyes shone like stars once more, reflecting the starry night sky that had inspired them all along.'}\n",
      "\n",
      "\n",
      "{'aggregator': 'Here\\'s a story, joke, and poem about cats!\\n\\nSTORY:\\nIn the quaint town of Whiskerville, there lived a community of felines who were known for their unique personalities and characteristics. Among them was a sleek black cat named Midnight, whose eyes shone like stars in the night sky.\\n\\nMidnight lived with her owner, an elderly woman named Mrs. Jenkins, in a cozy little house on the outskirts of town. Mrs. Jenkins adored Midnight and spoiled her rotten, feeding her treats and giving her endless belly rubs. In return, Midnight provided affection and companionship to Mrs. Jenkins, who had grown lonely after losing her husband.\\n\\nOne day, while exploring the attic of their home, Midnight stumbled upon an old trunk that had been hidden away for years. As she pawed at the lid, it creaked open, revealing a trove of forgotten treasures: glittering jewels, antique toys, and even an old catnip-filled toy mouse.\\n\\nAs Midnight began to investigate each item, she discovered that they were all connected to her own lineage. It turned out that Mrs. Jenkins\\' great-aunt had been a renowned cat breeder, and the trunk was filled with heirlooms from some of the most esteemed feline families in Whiskerville.\\n\\nMidnight\\'s curiosity was piqued, and she set out on a journey to explore the town and uncover more secrets about her ancestors. She began by visiting the local animal shelter, where she met a scrappy little calico named Ginger who claimed to have known Midnight\\'s great-great-grandmother.\\n\\nGinger told Midnight that their shared ancestor had been a fearless huntress, able to catch even the most elusive prey in the forest. Intrigued, Midnight continued her quest for knowledge, seeking out other felines with similar stories and experiences.\\n\\nHer travels took her through alleys, backyards, and hidden gardens, where she encountered an array of cats who shared tales of their own families\\' histories. There was Rufus, a majestic Maine Coon with piercing blue eyes; Luna, a gentle Persian with a silky coat and melodious voice; and even Jasper, a mischievous Siamese with a penchant for playing pranks on his owners.\\n\\nAs Midnight listened to each cat\\'s stories, she began to piece together a rich tapestry of Whiskerville\\'s feline heritage. She discovered that her ancestors had played a significant role in shaping the town\\'s history and culture, from their skillful hunting prowess to their expertise in weaving and craftsmanship.\\n\\nOver time, Midnight became known as the \"Whisker Historian,\" a title bestowed upon her by the cats of Whiskerville who revered her for her dedication to preserving their shared heritage. And with each new discovery, she grew more confident and curious, ready to uncover even more secrets about her own mysterious past.\\n\\nOne day, as Midnight curled up in Mrs. Jenkins\\' lap, she purred contentedly, knowing that she was part of a long line of remarkable cats who had shaped the town\\'s history and identity. And as she drifted off to sleep, her eyes shone like stars once more, reflecting the starry night sky that had inspired them all along.\\n\\nJOKE:\\nWhy did the cat join a band?\\n\\nBecause it wanted to be the purr-cussionist.\\n\\nPOEM:\\nWhiskered shadows dance and play,\\nMoonlight whispers, night\\'s sweet sway,\\nEyes like lanterns in the dark,\\nWatching worlds with secrets stark.\\n\\nSoft purrs rumble, velvet tone,\\nA soothing hum, a heart of stone,\\nTail twitches, ears perk high,\\nListening to the world go by.\\n\\nClaws unsheathed, a silent creep,\\nStalking prey in endless sleep,\\nThe huntress stirs, eyes aglow,\\nMidnight hunter, fierce and slow.\\n\\nTheir fur, a tapestry so fine,\\nPatterns woven like a shrine,\\n Colors shifting with each mood,\\nShades of gray, or eyes of gold.\\n\\nWith every step, a secret keeps,\\nA world of mystery, it seems to sleep,\\nBut when they curl up tight and still,\\nTheir hearts beat fast, their love unfulfilled.\\n\\nSo let us cherish these feline friends,\\nAnd honor the secrets that never end,\\nFor in their eyes, our souls are seen,\\nAnd in their purrs, a love is gleaned.'}\n",
      "\n",
      "\n",
      "{'parallel_workflow': 'Here\\'s a story, joke, and poem about cats!\\n\\nSTORY:\\nIn the quaint town of Whiskerville, there lived a community of felines who were known for their unique personalities and characteristics. Among them was a sleek black cat named Midnight, whose eyes shone like stars in the night sky.\\n\\nMidnight lived with her owner, an elderly woman named Mrs. Jenkins, in a cozy little house on the outskirts of town. Mrs. Jenkins adored Midnight and spoiled her rotten, feeding her treats and giving her endless belly rubs. In return, Midnight provided affection and companionship to Mrs. Jenkins, who had grown lonely after losing her husband.\\n\\nOne day, while exploring the attic of their home, Midnight stumbled upon an old trunk that had been hidden away for years. As she pawed at the lid, it creaked open, revealing a trove of forgotten treasures: glittering jewels, antique toys, and even an old catnip-filled toy mouse.\\n\\nAs Midnight began to investigate each item, she discovered that they were all connected to her own lineage. It turned out that Mrs. Jenkins\\' great-aunt had been a renowned cat breeder, and the trunk was filled with heirlooms from some of the most esteemed feline families in Whiskerville.\\n\\nMidnight\\'s curiosity was piqued, and she set out on a journey to explore the town and uncover more secrets about her ancestors. She began by visiting the local animal shelter, where she met a scrappy little calico named Ginger who claimed to have known Midnight\\'s great-great-grandmother.\\n\\nGinger told Midnight that their shared ancestor had been a fearless huntress, able to catch even the most elusive prey in the forest. Intrigued, Midnight continued her quest for knowledge, seeking out other felines with similar stories and experiences.\\n\\nHer travels took her through alleys, backyards, and hidden gardens, where she encountered an array of cats who shared tales of their own families\\' histories. There was Rufus, a majestic Maine Coon with piercing blue eyes; Luna, a gentle Persian with a silky coat and melodious voice; and even Jasper, a mischievous Siamese with a penchant for playing pranks on his owners.\\n\\nAs Midnight listened to each cat\\'s stories, she began to piece together a rich tapestry of Whiskerville\\'s feline heritage. She discovered that her ancestors had played a significant role in shaping the town\\'s history and culture, from their skillful hunting prowess to their expertise in weaving and craftsmanship.\\n\\nOver time, Midnight became known as the \"Whisker Historian,\" a title bestowed upon her by the cats of Whiskerville who revered her for her dedication to preserving their shared heritage. And with each new discovery, she grew more confident and curious, ready to uncover even more secrets about her own mysterious past.\\n\\nOne day, as Midnight curled up in Mrs. Jenkins\\' lap, she purred contentedly, knowing that she was part of a long line of remarkable cats who had shaped the town\\'s history and identity. And as she drifted off to sleep, her eyes shone like stars once more, reflecting the starry night sky that had inspired them all along.\\n\\nJOKE:\\nWhy did the cat join a band?\\n\\nBecause it wanted to be the purr-cussionist.\\n\\nPOEM:\\nWhiskered shadows dance and play,\\nMoonlight whispers, night\\'s sweet sway,\\nEyes like lanterns in the dark,\\nWatching worlds with secrets stark.\\n\\nSoft purrs rumble, velvet tone,\\nA soothing hum, a heart of stone,\\nTail twitches, ears perk high,\\nListening to the world go by.\\n\\nClaws unsheathed, a silent creep,\\nStalking prey in endless sleep,\\nThe huntress stirs, eyes aglow,\\nMidnight hunter, fierce and slow.\\n\\nTheir fur, a tapestry so fine,\\nPatterns woven like a shrine,\\n Colors shifting with each mood,\\nShades of gray, or eyes of gold.\\n\\nWith every step, a secret keeps,\\nA world of mystery, it seems to sleep,\\nBut when they curl up tight and still,\\nTheir hearts beat fast, their love unfulfilled.\\n\\nSo let us cherish these feline friends,\\nAnd honor the secrets that never end,\\nFor in their eyes, our souls are seen,\\nAnd in their purrs, a love is gleaned.'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@task\n",
    "def call_llm_1(topic: str):\n",
    "    \"\"\"First LLM call to generate initial joke\"\"\"\n",
    "    msg = llm.invoke(f\"Write a joke about {topic}\")\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "@task\n",
    "def call_llm_2(topic: str):\n",
    "    \"\"\"Second LLM call to generate story\"\"\"\n",
    "    msg = llm.invoke(f\"Write a story about {topic}\")\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "@task\n",
    "def call_llm_3(topic):\n",
    "    \"\"\"Third LLM call to generate poem\"\"\"\n",
    "    msg = llm.invoke(f\"Write a poem about {topic}\")\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "@task\n",
    "def aggregator(topic, joke, story, poem):\n",
    "    \"\"\"Combine the joke and story into a single output\"\"\"\n",
    "\n",
    "    combined = f\"Here's a story, joke, and poem about {topic}!\\n\\n\"\n",
    "    combined += f\"STORY:\\n{story}\\n\\n\"\n",
    "    combined += f\"JOKE:\\n{joke}\\n\\n\"\n",
    "    combined += f\"POEM:\\n{poem}\"\n",
    "    return combined\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "@entrypoint()\n",
    "def parallel_workflow(topic: str):\n",
    "    joke_fut = call_llm_1(topic)\n",
    "    story_fut = call_llm_2(topic)\n",
    "    poem_fut = call_llm_3(topic)\n",
    "    return aggregator(\n",
    "        topic, joke_fut.result(), story_fut.result(), poem_fut.result()\n",
    "    ).result()\n",
    "\n",
    "# Invoke\n",
    "for step in parallel_workflow.stream(\"cats\", stream_mode=\"updates\"):\n",
    "    print(step)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing\n",
    "\n",
    "Routing classifies an input and directs it to a followup task. As noted in the Anthropic blog:\n",
    "\n",
    "Routing classifies an input and directs it to a specialized followup task. This workflow allows for separation of concerns, and building more specialized prompts. Without this workflow, optimizing for one kind of input can hurt performance on other inputs.\n",
    "\n",
    "When to use this workflow: Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model/algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "# Schema for structured output to use as routing logic\n",
    "class Route(BaseModel):\n",
    "    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\n",
    "        None, description=\"The next step in the routing process\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "router = llm.with_structured_output(Route)\n",
    "\n",
    "\n",
    "@task\n",
    "def llm_call_1(input_: str):\n",
    "    \"\"\"Write a story\"\"\"\n",
    "    result = llm.invoke(input_)\n",
    "    return result.content\n",
    "\n",
    "\n",
    "@task\n",
    "def llm_call_2(input_: str):\n",
    "    \"\"\"Write a joke\"\"\"\n",
    "    result = llm.invoke(input_)\n",
    "    return result.content\n",
    "\n",
    "\n",
    "@task\n",
    "def llm_call_3(input_: str):\n",
    "    \"\"\"Write a poem\"\"\"\n",
    "    result = llm.invoke(input_)\n",
    "    return result.content\n",
    "\n",
    "\n",
    "def llm_call_router(input_: str):\n",
    "    \"\"\"Route the input to the appropriate node\"\"\"\n",
    "    # Run the augmented LLM with structured output to serve as routing logic\n",
    "    decision = router.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"Route the input to story, joke, or poem based on the user's request.\"\n",
    "            ),\n",
    "            HumanMessage(content=input_),\n",
    "        ]\n",
    "    )\n",
    "    return decision.step\n",
    "\n",
    "\n",
    "# Create workflow\n",
    "@entrypoint()\n",
    "def router_workflow(input_: str):\n",
    "    next_step = llm_call_router(input_)\n",
    "    if next_step == \"story\":\n",
    "        llm_call = llm_call_1\n",
    "    elif next_step == \"joke\":\n",
    "        llm_call = llm_call_2\n",
    "    elif next_step == \"poem\":\n",
    "        llm_call = llm_call_3\n",
    "\n",
    "    return llm_call(input_).result()\n",
    "\n",
    "# Invoke\n",
    "for step in router_workflow.stream(\"Write me a joke about cats\", stream_mode=\"updates\"):\n",
    "    print(step)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrator-Worker\n",
    "\n",
    "With orchestrator-worker, an orchestrator breaks down a task and delegates each sub-task to workers. As noted in the Anthropic blog:\n",
    "\n",
    "In the orchestrator-workers workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results.\n",
    "\n",
    "When to use this workflow: This workflow is well-suited for complex tasks where you can’t predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it’s topographically similar, the key difference from parallelization is its flexibility—subtasks aren't pre-defined, but determined by the orchestrator based on the specific input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "# Schema for structured output to use in planning\n",
    "class Section(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"Name for this section of the report.\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections: List[Section] = Field(\n",
    "        description=\"Sections of the report.\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "planner = llm.with_structured_output(Sections)\n",
    "\n",
    "\n",
    "@task\n",
    "def orchestrator(topic: str):\n",
    "    \"\"\"Orchestrator that generates a plan for the report\"\"\"\n",
    "    # Generate queries\n",
    "    report_sections = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Generate a plan for the report.\"),\n",
    "            HumanMessage(content=f\"Here is the report topic: {topic}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return report_sections.sections\n",
    "\n",
    "\n",
    "@task\n",
    "def llm_call(section: Section):\n",
    "    \"\"\"Worker writes a section of the report\"\"\"\n",
    "\n",
    "    # Generate section\n",
    "    result = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Write a report section.\"),\n",
    "            HumanMessage(\n",
    "                content=f\"Here is the section name: {section.name} and description: {section.description}\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Write the updated section to completed sections\n",
    "    return result.content\n",
    "\n",
    "\n",
    "@task\n",
    "def synthesizer(completed_sections: list[str]):\n",
    "    \"\"\"Synthesize full report from sections\"\"\"\n",
    "    final_report = \"\\n\\n---\\n\\n\".join(completed_sections)\n",
    "    return final_report\n",
    "\n",
    "\n",
    "@entrypoint()\n",
    "def orchestrator_worker(topic: str):\n",
    "    sections = orchestrator(topic).result()\n",
    "    section_futures = [llm_call(section) for section in sections]\n",
    "    final_report = synthesizer(\n",
    "        [section_fut.result() for section_fut in section_futures]\n",
    "    ).result()\n",
    "    return final_report\n",
    "\n",
    "# Invoke\n",
    "report = orchestrator_worker.invoke(\"Create a report on LLM scaling laws\")\n",
    "from IPython.display import Markdown\n",
    "Markdown(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator-optimizer\n",
    "\n",
    "In the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop:\n",
    "\n",
    "In the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop.\n",
    "\n",
    "When to use this workflow: This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema for structured output to use in evaluation\n",
    "class Feedback(BaseModel):\n",
    "    grade: Literal[\"funny\", \"not funny\"] = Field(\n",
    "        description=\"Decide if the joke is funny or not.\",\n",
    "    )\n",
    "    feedback: str = Field(\n",
    "        description=\"If the joke is not funny, provide feedback on how to improve it.\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "evaluator = llm.with_structured_output(Feedback)\n",
    "\n",
    "\n",
    "# Nodes\n",
    "@task\n",
    "def llm_call_generator(topic: str, feedback: Feedback):\n",
    "    \"\"\"LLM generates a joke\"\"\"\n",
    "    if feedback:\n",
    "        msg = llm.invoke(\n",
    "            f\"Write a joke about {topic} but take into account the feedback: {feedback}\"\n",
    "        )\n",
    "    else:\n",
    "        msg = llm.invoke(f\"Write a joke about {topic}\")\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "@task\n",
    "def llm_call_evaluator(joke: str):\n",
    "    \"\"\"LLM evaluates the joke\"\"\"\n",
    "    feedback = evaluator.invoke(f\"Grade the joke {joke}\")\n",
    "    return feedback\n",
    "\n",
    "\n",
    "@entrypoint()\n",
    "def optimizer_workflow(topic: str):\n",
    "    feedback = None\n",
    "    while True:\n",
    "        joke = llm_call_generator(topic, feedback).result()\n",
    "        feedback = llm_call_evaluator(joke).result()\n",
    "        if feedback.grade == \"funny\":\n",
    "            break\n",
    "\n",
    "    return joke\n",
    "\n",
    "# Invoke\n",
    "for step in optimizer_workflow.stream(\"Cats\", stream_mode=\"updates\"):\n",
    "    print(step)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "Agents are typically implemented as an LLM performing actions (via tool-calling) based on environmental feedback in a loop. As noted in the Anthropic blog:\n",
    "\n",
    "Agents can handle sophisticated tasks, but their implementation is often straightforward. They are typically just LLMs using tools based on environmental feedback in a loop. It is therefore crucial to design toolsets and their documentation clearly and thoughtfully.\n",
    "\n",
    "When to use agents: Agents can be used for open-ended problems where it’s difficult or impossible to predict the required number of steps, and where you can’t hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents' autonomy makes them ideal for scaling tasks in trusted environments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "# Define tools\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "# Augment the LLM with tools\n",
    "tools = [add, multiply, divide]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import add_messages\n",
    "from langchain_core.messages import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    BaseMessage,\n",
    "    ToolCall,\n",
    ")\n",
    "\n",
    "\n",
    "@task\n",
    "def call_llm(messages: list[BaseMessage]):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "    return llm_with_tools.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
    "            )\n",
    "        ]\n",
    "        + messages\n",
    "    )\n",
    "\n",
    "\n",
    "@task\n",
    "def call_tool(tool_call: ToolCall):\n",
    "    \"\"\"Performs the tool call\"\"\"\n",
    "    tool = tools_by_name[tool_call[\"name\"]]\n",
    "    return tool.invoke(tool_call)\n",
    "\n",
    "\n",
    "@entrypoint()\n",
    "def agent(messages: list[BaseMessage]):\n",
    "    llm_response = call_llm(messages).result()\n",
    "\n",
    "    while True:\n",
    "        if not llm_response.tool_calls:\n",
    "            break\n",
    "\n",
    "        # Execute tools\n",
    "        tool_result_futures = [\n",
    "            call_tool(tool_call) for tool_call in llm_response.tool_calls\n",
    "        ]\n",
    "        tool_results = [fut.result() for fut in tool_result_futures]\n",
    "        messages = add_messages(messages, [llm_response, *tool_results])\n",
    "        llm_response = call_llm(messages).result()\n",
    "\n",
    "    messages = add_messages(messages, llm_response)\n",
    "    return messages\n",
    "\n",
    "# Invoke\n",
    "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
    "for chunk in agent.stream(messages, stream_mode=\"updates\"):\n",
    "    print(chunk)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
